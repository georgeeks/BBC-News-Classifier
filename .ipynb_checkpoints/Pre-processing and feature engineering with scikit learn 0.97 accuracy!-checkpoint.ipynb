{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preprocessing with scikit learn"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Loading file from path\ndef loading_file():\n    file_dir = '/home/nbuser/library/1. Classifier/3. Exploratory Data Analysis'        \n    file_list = glob.glob(file_dir + '/*.csv')\n    csv_file = file_list[1]\n    return csv_file\n\n# Import file imto Pandas DataFrame\ndef importing_file(csv_file):\n    df = pd.read_csv(csv_file, sep=\",\")\n    return df\n\n# Saving path\ndef saving_file(file, file_name, save_dir):\n    file.to_csv(os.path.join(save_dir,file_name))\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Importing file + Loading  file\nnews_df = importing_file(loading_file())\n\n# Top 5 records\nnews_df.head()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "  file_name                              title  \\\n0   164.txt   Parker's saxophone heads auction   \n1   012.txt   Edwards tips Idowu for Euro gold   \n2   257.txt  Blair and Brown criticised by MPs   \n3   238.txt  Firms pump billions into pensions   \n4   400.txt   Monsanto fined $1.5m for bribery   \n\n                                           news_text       category  \n0  A saxophone belonging to legendary jazz musici...  entertainment  \n1  World outdoor triple jump record holder and BB...         sports  \n2  Labour MPs have angrily criticised Tony Blair ...       politics  \n3  Employers have spent billions of pounds proppi...       business  \n4  The US agrochemical giant Monsanto has agreed ...       business  ",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>title</th>\n      <th>news_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>164.txt</td>\n      <td>Parker's saxophone heads auction</td>\n      <td>A saxophone belonging to legendary jazz musici...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>012.txt</td>\n      <td>Edwards tips Idowu for Euro gold</td>\n      <td>World outdoor triple jump record holder and BB...</td>\n      <td>sports</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>257.txt</td>\n      <td>Blair and Brown criticised by MPs</td>\n      <td>Labour MPs have angrily criticised Tony Blair ...</td>\n      <td>politics</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>238.txt</td>\n      <td>Firms pump billions into pensions</td>\n      <td>Employers have spent billions of pounds proppi...</td>\n      <td>business</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>400.txt</td>\n      <td>Monsanto fined $1.5m for bribery</td>\n      <td>The US agrochemical giant Monsanto has agreed ...</td>\n      <td>business</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df.shape",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "(2003, 4)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df.category.unique()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "array(['entertainment', 'sports', 'politics', 'business', 'tech'],\n      dtype=object)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df.category.value_counts()",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "business         466\nsports           457\npolitics         368\ntech             364\nentertainment    348\nName: category, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Clean the news_text column"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\ndef clean_text(x):\n    pattern = r'[^a-zA-z0-9\\s]'\n    text = re.sub(pattern, '', x)\n    return text",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def clean_all_text(df):\n    for index, item in df.iterrows():\n        cleantext = clean_text(item['news_text'])\n        item['news_text'] = cleantext\n    return df",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df = clean_all_text(news_df)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Split the dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Create a series to store the labels: y\ny = news_df[\"category\"]\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(news_df[\"news_text\"], y, test_size=0.3, random_state=53)\n",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### CountVectorizer for text classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words=\"english\")\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test)\n\n            \n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names()[:100])\n",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['00', '000', '00051', '001', '003', '004secs', '007', '01', '0100', '0130', '019secs', '02', '0200', '022', '0227', '024', '025', '027', '028', '03', '030', '0300', '0305', '04', '040', '041', '048', '05', '053', '053bn', '05overall', '06', '0605', '0605festival', '0619', '07', '0700', '0710', '08', '0800', '083mph', '085', '0870', '089', '09', '090', '0900', '098', '099', '10', '100', '1000', '10000', '100000', '10000m', '10000vote', '1000bn', '1000m', '1000s', '1000th', '1001st', '1008', '100bn', '100m', '100mthey', '100s', '101', '101115', '10137', '1015', '1019', '101yearold', '102', '1020', '10216bn', '1022', '10227', '1025', '1026bn', '10276', '10280', '1028bn', '102inch', '102m', '103', '1030', '10360', '1038548recent', '1038am', '103bn', '103m', '104', '1040', '104000', '10416', '1044', '10499', '104bn', '104m', '105']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### TfidfVectorizer for text classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n    \nprint(tfidf_vectorizer.get_feature_names()[:100])",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['00', '000', '00051', '001', '003', '004secs', '007', '01', '0100', '0130', '019secs', '02', '0200', '022', '0227', '024', '025', '027', '028', '03', '030', '0300', '0305', '04', '040', '041', '048', '05', '053', '053bn', '05overall', '06', '0605', '0605festival', '0619', '07', '0700', '0710', '08', '0800', '083mph', '085', '0870', '089', '09', '090', '0900', '098', '099', '10', '100', '1000', '10000', '100000', '10000m', '10000vote', '1000bn', '1000m', '1000s', '1000th', '1001st', '1008', '100bn', '100m', '100mthey', '100s', '101', '101115', '10137', '1015', '1019', '101yearold', '102', '1020', '10216bn', '1022', '10227', '1025', '1026bn', '10276', '10280', '1028bn', '102inch', '102m', '103', '1030', '10360', '1038548recent', '1038am', '103bn', '103m', '104', '1040', '104000', '10416', '1044', '10499', '104bn', '104m', '105']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Show Vectors as Features"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the CountVectorizer DataFrame: count_df\ncount_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n\n# Create the TfidfVectorizer DataFrame: tfidf_df\ntfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n\n# Print the head of count_df\nprint(count_df.head())\nprint()\n# Print the head of tfidf_df\nprint(tfidf_df.head())\n\n# Calculate the difference in columns: difference\ndifference = set(count_df.columns) - set(tfidf_df.columns)\nprint(difference)\n\n# Check whether the DataFrames are equal\nprint(count_df.equals(tfidf_df))\n",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "   00  000  00051  001  003  004secs  007  01  0100  0130     ...       zones  \\\n0   0    0      0    0    0        0    0   0     0     0     ...           0   \n1   0    0      0    0    0        0    0   0     0     0     ...           0   \n2   0    0      0    0    0        0    0   0     0     0     ...           0   \n3   0    0      0    0    0        0    0   0     0     0     ...           0   \n4   0    0      0    0    0        0    0   0     0     0     ...           0   \n\n   zoom  zoomsmore  zorro  zubair  zuluaga  zurich  zutons  zvonareva  \\\n0     0          0      0       0        0       0       0          0   \n1     0          0      0       0        0       0       0          0   \n2     0          0      0       0        0       0       0          0   \n3     0          0      0       0        0       0       0          0   \n4     0          0      0       0        0       0       0          0   \n\n   zvyagintsev  \n0            0  \n1            0  \n2            0  \n3            0  \n4            0  \n\n[5 rows x 31334 columns]\n\n    00  000  00051  001  003  004secs  007   01  0100  0130     ...       \\\n0  0.0  0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n1  0.0  0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n2  0.0  0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n3  0.0  0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n4  0.0  0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n\n   zones  zoom  zoomsmore  zorro  zubair  zuluaga  zurich  zutons  zvonareva  \\\n0    0.0   0.0        0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n1    0.0   0.0        0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n2    0.0   0.0        0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n3    0.0   0.0        0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n4    0.0   0.0        0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n\n   zvyagintsev  \n0          0.0  \n1          0.0  \n2          0.0  \n3          0.0  \n4          0.0  \n\n[5 rows x 31333 columns]\n{'said'}\nFalse\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Training and testing the model with CountVectorizer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the necessary modules\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(count_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(count_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred, labels=['entertainment', 'politics', 'business', 'sports', 'tech'])\nprint(cm)\n",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.9717138103161398\n[[102   2   0   0   1]\n [  0 101   0   0   0]\n [  0   8 134   0   4]\n [  0   0   0 143   0]\n [  0   1   0   1 104]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Training and testing the model with TfidfVectorizer"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(tfidf_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred, labels=['entertainment', 'politics', 'business', 'sports', 'tech'])\nprint(cm)\n",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.9683860232945092\n[[100   3   1   0   1]\n [  0 101   0   0   0]\n [  0   5 139   0   2]\n [  0   0   0 143   0]\n [  0   2   2   3  99]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Improving your model\n"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create the list of alphas: alphas\nalphas = np.arange(0, 1, .1)\n\n# Define train_and_predict()\ndef train_and_predict(alpha):\n    # Instantiate the classifier: nb_classifier\n    nb_classifier = MultinomialNB(alpha=alpha)\n    # Fit to the training data\n    nb_classifier.fit(tfidf_train, y_train)\n    # Predict the labels: pred\n    pred = nb_classifier.predict(tfidf_test)\n    # Compute accuracy: score\n    score = metrics.accuracy_score(y_test, pred)\n    return score\n\n# Iterate over the alphas and print the corresponding score\nbest_alfa_score = {}\nfor alpha in alphas:\n    best_alfa_score[alpha] = train_and_predict(alpha)\n    print('Alpha: ', alpha)\n    print('Score: ', train_and_predict(alpha))\n    print()\nbest_score = max(best_alfa_score, key=best_alfa_score.get)\n\nprint(\"Best alfa: \", best_score, \"best accuracy: \",best_alfa_score[best_score])",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n  'setting alpha = %.1e' % _ALPHA_MIN)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Alpha:  0.0\nScore:  0.9683860232945092\n\nAlpha:  0.1\nScore:  0.9750415973377704\n\nAlpha:  0.2\nScore:  0.9750415973377704\n\nAlpha:  0.30000000000000004\nScore:  0.9750415973377704\n\nAlpha:  0.4\nScore:  0.9717138103161398\n\nAlpha:  0.5\nScore:  0.9733777038269551\n\nAlpha:  0.6000000000000001\nScore:  0.9700499168053245\n\nAlpha:  0.7000000000000001\nScore:  0.9700499168053245\n\nAlpha:  0.8\nScore:  0.9683860232945092\n\nAlpha:  0.9\nScore:  0.9683860232945092\n\nBest alfa:  0.1 best accuracy:  0.9750415973377704\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create the list of alphas: alphas\nalphas = np.arange(0, 1, .1)\n\n# Define train_and_predict()\ndef train_and_predict(alpha):\n    # Instantiate the classifier: nb_classifier\n    nb_classifier = MultinomialNB(alpha=alpha)\n    # Fit to the training data\n    nb_classifier.fit(count_train, y_train)\n    # Predict the labels: pred\n    pred = nb_classifier.predict(count_test)\n    # Compute accuracy: score\n    score = metrics.accuracy_score(y_test, pred)\n    return score\n\n# Iterate over the alphas and print the corresponding score\nbest_alfa_score = {}\nfor alpha in alphas:\n    best_alfa_score[alpha] = train_and_predict(alpha)\n    print('Alpha: ', alpha)\n    print('Score: ', train_and_predict(alpha))\n    print()\nbest_score = max(best_alfa_score, key=best_alfa_score.get)\n\nprint(\"Best alfa: \", best_score, \"best accuracy: \",best_alfa_score[best_score])",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n  'setting alpha = %.1e' % _ALPHA_MIN)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Alpha:  0.0\nScore:  0.9717138103161398\n\nAlpha:  0.1\nScore:  0.9816971713810316\n\nAlpha:  0.2\nScore:  0.9800332778702163\n\nAlpha:  0.30000000000000004\nScore:  0.978369384359401\n\nAlpha:  0.4\nScore:  0.978369384359401\n\nAlpha:  0.5\nScore:  0.978369384359401\n\nAlpha:  0.6000000000000001\nScore:  0.9733777038269551\n\nAlpha:  0.7000000000000001\nScore:  0.9717138103161398\n\nAlpha:  0.8\nScore:  0.9717138103161398\n\nAlpha:  0.9\nScore:  0.9717138103161398\n\nBest alfa:  0.1 best accuracy:  0.9816971713810316\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Inspecting the model\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the class labels: class_labels\nclass_labels = nb_classifier.classes_\n\n# Extract the features: feature_names\nfeature_names = tfidf_vectorizer.get_feature_names()\n\n# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\nfeat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n\n# Print the first class label and the top 20 feat_with_weights entries\nprint(class_labels[0], feat_with_weights[:20])\n\n# Print the second class label and the bottom 20 feat_with_weights entries\nprint(class_labels[1], feat_with_weights[-20:])\n",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "business [(-10.45661585751269, '00'), (-10.45661585751269, '000'), (-10.45661585751269, '0001'), (-10.45661585751269, '00051'), (-10.45661585751269, '002'), (-10.45661585751269, '004secs'), (-10.45661585751269, '007'), (-10.45661585751269, '0100'), (-10.45661585751269, '0130'), (-10.45661585751269, '019secs'), (-10.45661585751269, '0227'), (-10.45661585751269, '028'), (-10.45661585751269, '0305'), (-10.45661585751269, '040'), (-10.45661585751269, '0469'), (-10.45661585751269, '053bn'), (-10.45661585751269, '0800'), (-10.45661585751269, '0870'), (-10.45661585751269, '090'), (-10.45661585751269, '1000000000')]\nentertainment [(-8.613260493776794, 'stock'), (-8.60427307626571, 'new'), (-8.577506079896372, 'firms'), (-8.57157225931154, 'rise'), (-8.566396008929635, 'government'), (-8.546549679980423, 'yukos'), (-8.370228484903935, '2004'), (-8.365628689869363, 'prices'), (-8.352785134212938, 'firm'), (-8.311158085240452, 'economic'), (-8.297926644109499, 'shares'), (-8.282300993198296, 'mr'), (-8.243739393210502, 'oil'), (-8.202936389020412, 'company'), (-8.198543163217678, 'economy'), (-8.166558324816604, 'sales'), (-8.141683196044053, 'year'), (-8.12592853179391, 'market'), (-8.122109102459538, 'bank'), (-8.105312532026895, 'growth')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### Test on new dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Import new data\nos.chdir('/home/nbuser/library/1. Classifier/3. Exploratory Data Analysis')\nnew_data = pd.read_csv('news_df_validation.csv')\nnew_data.head(10)",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "  file_name                              title  \\\n0   214.txt        Mansfield 0-1 Leyton Orient   \n1   303.txt  Film production 'falls' 40% in UK   \n2   083.txt   Hague 'given up' his PM ambition   \n3   190.txt             SA return to Mauritius   \n4   103.txt    Minimum rate for foster parents   \n5   088.txt    Ocean's Twelve raids box office   \n6   155.txt   Disputed Nirvana box set on sale   \n7   077.txt           DVD review: Spider-Man 2   \n8   227.txt    Spam e-mails tempt net shoppers   \n9   039.txt  Chinese wine tempts Italy's Illva   \n\n                                           news_text       category  \n0  An second-half goal from Andy Scott condemned ...         sports  \n1  The number of British films produced in the UK...  entertainment  \n2  Former Conservative leader William Hague says ...       politics  \n3  Top seeds South Africa return to the scene of ...         sports  \n4  Foster carers are to be guaranteed a minimum a...       politics  \n5  Ocean's Twelve, the crime caper sequel starrin...  entertainment  \n6  A box set featuring 68 unreleased Nirvana trac...  entertainment  \n7  It's a universal rule that a film can either b...  entertainment  \n8  Computer users across the world continue to ig...           tech  \n9  Italy's Illva Saronno has agreed to buy 33% of...       business  ",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>title</th>\n      <th>news_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>214.txt</td>\n      <td>Mansfield 0-1 Leyton Orient</td>\n      <td>An second-half goal from Andy Scott condemned ...</td>\n      <td>sports</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>303.txt</td>\n      <td>Film production 'falls' 40% in UK</td>\n      <td>The number of British films produced in the UK...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>083.txt</td>\n      <td>Hague 'given up' his PM ambition</td>\n      <td>Former Conservative leader William Hague says ...</td>\n      <td>politics</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>190.txt</td>\n      <td>SA return to Mauritius</td>\n      <td>Top seeds South Africa return to the scene of ...</td>\n      <td>sports</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>103.txt</td>\n      <td>Minimum rate for foster parents</td>\n      <td>Foster carers are to be guaranteed a minimum a...</td>\n      <td>politics</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>088.txt</td>\n      <td>Ocean's Twelve raids box office</td>\n      <td>Ocean's Twelve, the crime caper sequel starrin...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>155.txt</td>\n      <td>Disputed Nirvana box set on sale</td>\n      <td>A box set featuring 68 unreleased Nirvana trac...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>077.txt</td>\n      <td>DVD review: Spider-Man 2</td>\n      <td>It's a universal rule that a film can either b...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>227.txt</td>\n      <td>Spam e-mails tempt net shoppers</td>\n      <td>Computer users across the world continue to ig...</td>\n      <td>tech</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>039.txt</td>\n      <td>Chinese wine tempts Italy's Illva</td>\n      <td>Italy's Illva Saronno has agreed to buy 33% of...</td>\n      <td>business</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "new_data.category.value_counts()",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "business         56\npolitics         49\nsports           48\ntech             36\nentertainment    33\nName: category, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Count Vectorizer on test dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def new_data_count_vec_predict(test_new_data):\n    clean_df = clean_all_text(test_new_data)\n    nb_classifier = MultinomialNB()\n    nb_classifier.fit(count_train, y_train)\n    count_test = count_vectorizer.transform(test_new_data['news_text'])\n    new_cv_pred = nb_classifier.predict(count_test)\n    return new_cv_pred",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def print_new_data_pred(test_data, index):\n    new_data_cat = new_data['category']\n    print(new_data_count_vec_predict(test_data)[index],' == ', new_data_cat[index])",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print_new_data_pred(new_data, 118)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "sports  ==  sports\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count_vec_new_pred = new_data_count_vec_predict(new_data)",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### TFIDF new dataset test"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def new_data_ftidf_vec_predict(test_new_data):\n    clean_df = clean_all_text(test_new_data)\n    nb_classifier = MultinomialNB()\n    nb_classifier.fit(tfidf_train, y_train)\n    ftidf_test = tfidf_vectorizer.transform(test_new_data['news_text'])\n    new_tfidf_pred = nb_classifier.predict(ftidf_test)\n    return new_tfidf_pred",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tfidf_new_pred = new_data_ftidf_vec_predict(new_data)",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(tfidf_new_pred[112])",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": "sports\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count_vec_score = metrics.accuracy_score(count_vec_new_pred, new_data['category'])\nprint('Count vectorize score: ',count_vec_score)\nprint()\ntfidf_vec_score = metrics.accuracy_score(new_data_ftidf_vec_predict(new_data), new_data['category'])\nprint('TF-IDF score: ',tfidf_vec_score)",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Count vectorize score:  0.9774774774774775\n\nTF-IDF score:  0.972972972972973\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### What the... Score! CV: 0.9774774774774775, TFIDF: 0.972972972972973"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}