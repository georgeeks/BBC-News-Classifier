{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Cleaning and Tokenization data using nltk, re"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the regex module\nimport re\n\n# Write a pattern to match sentence endings: sentence_endings\n# sentence_endings = r\"[.?!]\"\n\n# Split string on sentence endings and print the result\n# print(re.split(sentence_endings, string))\n\n# Find all capitalized words in string and print the result\ncapitalized_words = r\"[A-Z]\\w+\"\nprint(re.findall(capitalized_words, string))\n\n# Split string on spaces and print the result\nspaces = r\"\\s+\"\nprint(re.split(spaces, string))\n\n# Find all digits in string and print the result\ndigits = r\"\\d+\"\nprint(re.findall(digits, string))\n\n# Match only words and digits\nmatch_digits_and_words = ('(\\d+|\\w+)')\n\n\nre.search(example, string)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Examples re expressions"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "[A-Za-z]+\tupper and lowercase English alphabet\t'ABCDEFghijk'\n[0-9]\tnumbers from 0 to 9\t9\n[A-Za-z\\-\\.]+\tupper and lowercase English alphabet, - and .\t'My-Website.com'\n(a-z)\ta, - and z\t'a-z'\n(\\s+l,)\tspaces or a comma\t', '",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Tokenize sentences and words"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk, pprint",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import necessary modules\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n# Split scene_one into sentences: sentences\nsentences = sent_tokenize(text)\n\n# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\ntokenized_sent = word_tokenize(sentences[3])\n\n# Make a set of unique tokens in the entire scene: unique_tokens\nunique_tokens = set(word_tokenize(text))\n\nprint(unique_tokens)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Examples"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define hashtags pattern\nhashtag_pattern = r\"#\\w+\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open('003-Copy.txt') as text:\n    file = text.read()",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Preprocess with lematization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\n\n# Retain alphabetic words: alpha_only\nalpha_only = [t for t in lower_tokens if t.isalpha()]\n\n# Remove all stop words: no_stops\nno_stops = [t for t in alpha_only if t not in english_stops]\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preprocess with spaCy"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import spacy",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Tokenizing the Text\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the pipeline 'sentencizer' component\nsbd = nlp.create_pipe('sentencizer')\n\n# Add the component to the pipeline\nnlp.add_pipe(sbd)\n\ntext = \"\"\"When learning data science, you shouldn't get discouraged!\nChallenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n\n#  \"nlp\" Object is used to create documents with linguistic annotations.\ndoc = nlp(text)\n\n# create list of sentence tokens\nsents_list = []\nfor sent in doc.sents:\n    sents_list.append(sent.text)\nprint(sents_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# sentence tokenization\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = English()\n\n# Create the pipeline 'sentencizer' component\nsbd = nlp.create_pipe('sentencizer')\n\n# Add the component to the pipeline\nnlp.add_pipe(sbd)\n\ntext = \"\"\"When learning data science, you shouldn't get discouraged!\nChallenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n\n#  \"nlp\" Object is used to create documents with linguistic annotations.\ndoc = nlp(text)\n\n# create list of sentence tokens\nsents_list = []\nfor sent in doc.sents:\n    sents_list.append(sent.text)\nprint(sents_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Cleaning Text Data: Removing Stopwords"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Stop words\n#importing stop words from English language.\nimport spacy\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\n#Printing the total number of stop words:\nprint('Number of stop words: %d' % len(spacy_stopwords))\n\n#Printing first ten stop words:\nprint('First ten stop words: %s' % list(spacy_stopwords)[:20])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Removing Stopwords from Our Data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from spacy.lang.en.stop_words import STOP_WORDS\n\n#Implementation of stop words:\nfiltered_sent=[]\n\n#  \"nlp\" Object is used to create documents with linguistic annotations.\ndoc = nlp(text)\n\n# filtering stop words\nfor word in doc:\n    if word.is_stop==False:\n        filtered_sent.append(word)\nprint(\"Filtered Sentence:\",filtered_sent)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Lexicon Normalization"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Lemmatization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Implementing lemmatization\nlem = nlp(\"run runs running runner\")\n# finding lemma for each word\nfor word in lem:\n    print(word.text,word.lemma_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Part of Speech (POS) Tagging"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# POS tagging\n\n# importing the model en_core_web_sm of English for vocabluary, syntax & entities\nimport en_core_web_sm\n\n# load en_core_web_sm of English for vocabluary, syntax & entities\nnlp = en_core_web_sm.load()\n\n#  \"nlp\" Objectis used to create documents with linguistic annotations.\ndocs = nlp(u\"All is well that ends well.\")\n\nfor word in docs:\n    print(word.text,word.pos_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Entity Detection"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#for visualization of Entity detection importing displacy from spacy:\n\nfrom spacy import displacy\n\nnytimes= nlp(u\"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n\nAt least 285 people have contracted measles in the city since September, mostly in Brooklynâ€™s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n\nThe mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")\n\nentities=[(i, i.label_, i.label) for i in nytimes.ents]\nentities",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "displacy.render(nytimes, style = \"ent\",jupyter = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Dependency Parsing"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "docp = nlp (\" In pursuit of a wall, President Trump ran into one.\")\n\nfor chunk in docp.noun_chunks:\n   print(chunk.text, chunk.root.text, chunk.root.dep_,\n          chunk.root.head.text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Word Vector Representation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import en_core_web_sm\nnlp = en_core_web_sm.load()\nmango = nlp(u'mango')\nprint(mango.vector.shape)\nprint(mango.vector)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### SpaCy Text Classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "### Importing Libraries",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preprocessing with schikit learn"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count_vectorizer = CountVectorizer(stop_words=\"english\")",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df[\"label\"]\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size=0.33, random_state=53)\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words=\"english\")\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test)\n\n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names()[:10])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\nprint(tfidf_vectorizer.get_feature_names()[:10])\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.A[:5])\n\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}