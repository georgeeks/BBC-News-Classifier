{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preprocessing with scikit learn"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Loading file from path\ndef loading_file():\n    file_dir = '/home/nbuser/library/1. Classifier/3. Exploratory Data Analysis'        \n    file_list = glob.glob(file_dir + '/*.csv')\n    csv_file = file_list[0]\n    return csv_file\n\n# Import file imto Pandas DataFrame\ndef importing_file(csv_file):\n    df = pd.read_csv(csv_file, sep=\",\")\n    return df\n\n# Saving path\ndef saving_file(file, file_name, save_dir):\n    file.to_csv(os.path.join(save_dir,file_name))\n",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing file + Loading  file\nnews_df = importing_file(loading_file())\n\n# Top 5 records\nnews_df.head()",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>title</th>\n      <th>news_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>348.txt</td>\n      <td>Berlin celebrates European cinema</td>\n      <td>Organisers say this year's Berlin Film Festiva...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>139.txt</td>\n      <td>U2 to play at Grammy awards show</td>\n      <td>Irish rock band U2 are to play live at the Gra...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>125.txt</td>\n      <td>Snow Patrol feted at Irish awards</td>\n      <td>Snow Patrol were the big winners in Ireland's ...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>267.txt</td>\n      <td>T in the Park sells out in days</td>\n      <td>Tickets for Scotland's biggest music festival ...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>311.txt</td>\n      <td>Corbett attacks 'dumbed-down TV'</td>\n      <td>Ronnie Corbett has joined fellow comedy stars ...</td>\n      <td>entertainment</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "  file_name                              title  \\\n0   348.txt  Berlin celebrates European cinema   \n1   139.txt   U2 to play at Grammy awards show   \n2   125.txt  Snow Patrol feted at Irish awards   \n3   267.txt    T in the Park sells out in days   \n4   311.txt   Corbett attacks 'dumbed-down TV'   \n\n                                           news_text       category  \n0  Organisers say this year's Berlin Film Festiva...  entertainment  \n1  Irish rock band U2 are to play live at the Gra...  entertainment  \n2  Snow Patrol were the big winners in Ireland's ...  entertainment  \n3  Tickets for Scotland's biggest music festival ...  entertainment  \n4  Ronnie Corbett has joined fellow comedy stars ...  entertainment  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df.category.unique()",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "array(['entertainment', 'politics', 'business', 'sports', 'tech'],\n      dtype=object)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Clean the news_text column"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\ndef clean_text(x):\n    pattern = r'[^a-zA-z0-9\\s]'\n    text = re.sub(pattern, '', x)\n    return text",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def clean_all_text(df):\n    for index, item in df.iterrows():\n        cleantext = clean_text(item['news_text'])\n        item['news_text'] = cleantext\n        \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "clean_df = clean_all_text(news_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(news_df.iloc[22, 2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### CountVectorizer for text classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# Create a series to store the labels: y\ny = news_df[\"category\"]\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(news_df[\"news_text\"], y, test_size=0.3, random_state=53)\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words=\"english\")\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test)\n\n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names()[:10])\n",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['00', '000', '0001', '000m', '000s', '000th', '0051', '007', '01', '0100']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### TfidfVectorizer for text classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\nprint(tfidf_vectorizer.get_feature_names()[:10])\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.A[:5])\n",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['00', '000', '0001', '000m', '000s', '000th', '0051', '007', '01', '0100']\n[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.03914722 0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Inspecting the vectors"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the CountVectorizer DataFrame: count_df\ncount_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n\n# Create the TfidfVectorizer DataFrame: tfidf_df\ntfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n\n# Print the head of count_df\nprint(count_df.head())\n\n# Print the head of tfidf_df\nprint(tfidf_df.head())\n\n# Calculate the difference in columns: difference\ndifference = set(count_df.columns) - set(tfidf_df.columns)\nprint(difference)\n\n# Check whether the DataFrames are equal\nprint(count_df.equals(tfidf_df))\n",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "   00  000  0001  000m  000s  000th  0051  007  01  0100     ...       zones  \\\n0   0    0     0     0     0      0     0    0   0     0     ...           0   \n1   0    0     0     0     0      0     0    0   0     0     ...           0   \n2   0    1     0     0     0      0     0    0   0     0     ...           0   \n3   0    0     0     0     0      0     0    0   0     0     ...           0   \n4   0    0     0     0     0      0     0    0   0     0     ...           0   \n\n   zoom  zooms  zooropa  zornotza  zubair  zurich  zutons  zvonareva  \\\n0     0      0        0         0       0       0       0          0   \n1     0      0        0         0       0       0       0          0   \n2     0      0        0         0       0       0       0          0   \n3     0      0        0         0       0       0       0          0   \n4     0      0        0         0       0       0       0          0   \n\n   zvyagintsev  \n0            0  \n1            0  \n2            0  \n3            0  \n4            0  \n\n[5 rows x 23817 columns]\n    00       000  0001  000m  000s  000th  0051  007   01  0100     ...       \\\n0  0.0  0.000000   0.0   0.0   0.0    0.0   0.0  0.0  0.0   0.0     ...        \n1  0.0  0.000000   0.0   0.0   0.0    0.0   0.0  0.0  0.0   0.0     ...        \n2  0.0  0.039147   0.0   0.0   0.0    0.0   0.0  0.0  0.0   0.0     ...        \n3  0.0  0.000000   0.0   0.0   0.0    0.0   0.0  0.0  0.0   0.0     ...        \n4  0.0  0.000000   0.0   0.0   0.0    0.0   0.0  0.0  0.0   0.0     ...        \n\n   zones  zoom  zooms  zooropa  zornotza  zubair  zurich  zutons  zvonareva  \\\n0    0.0   0.0    0.0      0.0       0.0     0.0     0.0     0.0        0.0   \n1    0.0   0.0    0.0      0.0       0.0     0.0     0.0     0.0        0.0   \n2    0.0   0.0    0.0      0.0       0.0     0.0     0.0     0.0        0.0   \n3    0.0   0.0    0.0      0.0       0.0     0.0     0.0     0.0        0.0   \n4    0.0   0.0    0.0      0.0       0.0     0.0     0.0     0.0        0.0   \n\n   zvyagintsev  \n0          0.0  \n1          0.0  \n2          0.0  \n3          0.0  \n4          0.0  \n\n[5 rows x 23816 columns]\n{'said'}\nFalse\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Training and testing the \"fake news\" model with CountVectorizer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the necessary modules\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(count_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(count_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred, labels=['entertainment', 'politics', 'business', 'sports', 'tech'])\nprint(cm)\n",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.9733777038269551\n[[ 43   1   1   0   3]\n [  0 129   3   0   1]\n [  0   0 138   0   4]\n [  0   0   1 152   0]\n [  1   1   0   0 123]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Training and testing the \"fake news\" model with TfidfVectorizer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(tfidf_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred, labels=['entertainment', 'politics', 'business', 'sports', 'tech'])\nprint(cm)\n",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.913477537437604\n[[ 10   5   8  17   8]\n [  0 128   4   0   1]\n [  0   0 139   0   3]\n [  0   0   1 152   0]\n [  0   2   3   0 120]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Improving your model\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the list of alphas: alphas\nalphas = np.arange(0, 1, .1)\n\n# Define train_and_predict()\ndef train_and_predict(alpha):\n    # Instantiate the classifier: nb_classifier\n    nb_classifier = MultinomialNB(alpha=alpha)\n    # Fit to the training data\n    nb_classifier.fit(tfidf_train, y_train)\n    # Predict the labels: pred\n    pred = nb_classifier.predict(tfidf_test)\n    # Compute accuracy: score\n    score = metrics.accuracy_score(y_test, pred)\n    return score\n\n# Iterate over the alphas and print the corresponding score\nbest_alfa_score = {}\nfor alpha in alphas:\n    best_alfa_score[alpha] = train_and_predict(alpha)\n    print('Alpha: ', alpha)\n    print('Score: ', train_and_predict(alpha))\n    print()\nbest_score = max(best_alfa_score, key=best_alfa_score.get)\n\nprint(\"Best alfa: \", best_score, \"best accuracy: \",best_alfa_score[best_score])",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n  'setting alpha = %.1e' % _ALPHA_MIN)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Alpha:  0.0\nScore:  0.9550748752079867\n\nAlpha:  0.1\nScore:  0.9717138103161398\n\nAlpha:  0.2\nScore:  0.9550748752079867\n\nAlpha:  0.30000000000000004\nScore:  0.9450915141430949\n\nAlpha:  0.4\nScore:  0.9334442595673876\n\nAlpha:  0.5\nScore:  0.9284525790349417\n\nAlpha:  0.6000000000000001\nScore:  0.9284525790349417\n\nAlpha:  0.7000000000000001\nScore:  0.9234608985024958\n\nAlpha:  0.8\nScore:  0.9217970049916805\n\nAlpha:  0.9\nScore:  0.9217970049916805\n\nBest alfa:  0.1 best accuracy:  0.9717138103161398\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Inspecting the model\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the class labels: class_labels\nclass_labels = nb_classifier.classes_\n\n# Extract the features: feature_names\nfeature_names = tfidf_vectorizer.get_feature_names()\n\n# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\nfeat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n\n# Print the first class label and the top 20 feat_with_weights entries\nprint(class_labels[0], feat_with_weights[:20])\n\n# Print the second class label and the bottom 20 feat_with_weights entries\nprint(class_labels[1], feat_with_weights[-20:])\n",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": "business [(-10.204576282558833, '00'), (-10.204576282558833, '0001'), (-10.204576282558833, '000m'), (-10.204576282558833, '000s'), (-10.204576282558833, '000th'), (-10.204576282558833, '0051'), (-10.204576282558833, '007'), (-10.204576282558833, '0100'), (-10.204576282558833, '0130'), (-10.204576282558833, '02'), (-10.204576282558833, '0227'), (-10.204576282558833, '028'), (-10.204576282558833, '033'), (-10.204576282558833, '04m'), (-10.204576282558833, '04secs'), (-10.204576282558833, '056'), (-10.204576282558833, '05m'), (-10.204576282558833, '060'), (-10.204576282558833, '072'), (-10.204576282558833, '080')]\nentertainment [(-8.36079612343115, 'rates'), (-8.341688436601189, 'december'), (-8.33235401825379, 'rise'), (-8.229722130061955, 'government'), (-8.218499183038658, 'yukos'), (-8.178871053906013, 'china'), (-8.106851667128925, 'prices'), (-8.074253466673845, '2004'), (-8.069045380351342, 'shares'), (-8.052534937301782, 'economic'), (-8.042430513612883, 'firm'), (-8.002019779531608, 'oil'), (-7.981616663290847, 'mr'), (-7.900496502271874, 'economy'), (-7.899511203760827, 'sales'), (-7.8711111160591685, 'company'), (-7.859381285388077, 'year'), (-7.8520954264137846, 'market'), (-7.83070563430621, 'growth'), (-7.805412986512593, 'bank')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}