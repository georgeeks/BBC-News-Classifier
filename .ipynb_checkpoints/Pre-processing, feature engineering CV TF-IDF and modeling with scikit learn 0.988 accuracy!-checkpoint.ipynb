{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preprocessing with scikit learn"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Loading file from path\ndef loading_file():\n    file_dir = '/home/nbuser/library/1. Classifier/3. Exploratory Data Analysis'        \n    file_list = glob.glob(file_dir + '/*.csv')\n    csv_file = file_list[1]\n    return csv_file\n\n# Import file imto Pandas DataFrame\ndef importing_file(csv_file):\n    df = pd.read_csv(csv_file, sep=\",\")\n    return df\n\n# Saving path\ndef saving_file(file, file_name, save_dir):\n    file.to_csv(os.path.join(save_dir,file_name))\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Importing file + Loading  file\nnews_df = importing_file(loading_file())\n\n# Top 5 records\nnews_df.head()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>title</th>\n      <th>news_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>047.txt</td>\n      <td>Berlin applauds Hotel Rwanda</td>\n      <td>Political thriller Hotel Rwanda was given a ro...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>355.txt</td>\n      <td>Wales get Williams fitness boost</td>\n      <td>Wales are hopeful that openside flanker Martyn...</td>\n      <td>sports</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>481.txt</td>\n      <td>Christmas sales worst since 1981</td>\n      <td>UK retail sales fell in December, failing to m...</td>\n      <td>business</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>376.txt</td>\n      <td>MPs demand 'Budget leak' answers</td>\n      <td>Ministers have been asked to explain how Budge...</td>\n      <td>politics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>044.txt</td>\n      <td>US actor Ossie Davis found dead</td>\n      <td>US actor Ossie Davis has been found dead at th...</td>\n      <td>entertainment</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "  file_name                             title  \\\n0   047.txt      Berlin applauds Hotel Rwanda   \n1   355.txt  Wales get Williams fitness boost   \n2   481.txt  Christmas sales worst since 1981   \n3   376.txt  MPs demand 'Budget leak' answers   \n4   044.txt   US actor Ossie Davis found dead   \n\n                                           news_text       category  \n0  Political thriller Hotel Rwanda was given a ro...  entertainment  \n1  Wales are hopeful that openside flanker Martyn...         sports  \n2  UK retail sales fell in December, failing to m...       business  \n3  Ministers have been asked to explain how Budge...       politics  \n4  US actor Ossie Davis has been found dead at th...  entertainment  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df.shape",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "(2003, 4)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df.category.value_counts()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "sports           463\nbusiness         454\npolitics         368\ntech             365\nentertainment    353\nName: category, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Clean the news_text column"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\ndef clean_text(x):\n    pattern = r'[^a-zA-z0-9\\s]'\n    text = re.sub(pattern, '', str(x))\n    return text\n\ndef clean_all_text(df):\n    for index, item in df.iterrows():\n        cleantext = clean_text(item['news_text'])\n        item['news_text'] = cleantext\n    return df",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df = clean_all_text(news_df)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Split the dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Create a series to store the labels: y\ny = news_df[\"category\"]\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(news_df[\"news_text\"], y, test_size=0.3, random_state=53)\n",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### CountVectorizer for text classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words=\"english\")\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test)\n\n            \n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names()[:100])\n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['00', '0001', '00051', '002', '003', '004secs', '007', '01', '0100', '0130', '019secs', '02', '0227', '024', '025', '027', '028', '03', '030', '0300', '0305', '04', '0400', '041', '0469', '05', '050505', '053', '053bn', '05overall', '06', '0605', '0619', '067', '07', '0700', '0708', '0710', '0730', '08', '0800', '083mph', '0845', '0870', '09', '090', '0900', '0950', '099', '10', '100', '1000', '10000', '100000', '1000000000', '100000ayear', '10000m', '10000vote', '1000m', '1000th', '1001st', '1008', '100bn', '100m', '100s', '101', '101115', '1012', '1013', '10137', '1015', '1019', '101yearold', '102', '1020', '1022', '10227', '1025', '10255', '1025m', '10280', '102975', '102inch', '102m', '103', '1030', '10305', '103500', '1038am', '103bn', '103m', '104', '104000', '10416', '1044', '10476', '1048', '104bn', '104m', '105']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### TfidfVectorizer for text classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words=\"english\",max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n    \nprint(tfidf_vectorizer.get_feature_names()[:100])",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['00', '0001', '00051', '002', '003', '004secs', '007', '01', '0100', '0130', '019secs', '02', '0227', '024', '025', '027', '028', '03', '030', '0300', '0305', '04', '0400', '041', '0469', '05', '050505', '053', '053bn', '05overall', '06', '0605', '0619', '067', '07', '0700', '0708', '0710', '0730', '08', '0800', '083mph', '0845', '0870', '09', '090', '0900', '0950', '099', '10', '100', '1000', '10000', '100000', '1000000000', '100000ayear', '10000m', '10000vote', '1000m', '1000th', '1001st', '1008', '100bn', '100m', '100s', '101', '101115', '1012', '1013', '10137', '1015', '1019', '101yearold', '102', '1020', '1022', '10227', '1025', '10255', '1025m', '10280', '102975', '102inch', '102m', '103', '1030', '10305', '103500', '1038am', '103bn', '103m', '104', '104000', '10416', '1044', '10476', '1048', '104bn', '104m', '105']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Show Vectors as Features"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the CountVectorizer DataFrame: count_df\ncount_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n\n# Create the TfidfVectorizer DataFrame: tfidf_df\ntfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n\n# Print the head of count_df\nprint(count_df.head())\nprint()\n# Print the head of tfidf_df\nprint(tfidf_df.head())\n\n# Calculate the difference in columns: difference\ndifference = set(count_df.columns) - set(tfidf_df.columns)\nprint(difference)\n\n# Check whether the DataFrames are equal\nprint(count_df.equals(tfidf_df))\n",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "   00  0001  00051  002  003  004secs  007  01  0100  0130     ...       \\\n0   0     0      0    0    0        0    0   0     0     0     ...        \n1   0     0      0    0    0        0    0   0     0     0     ...        \n2   0     0      0    0    0        0    0   0     0     0     ...        \n3   0     0      0    0    0        0    0   0     0     0     ...        \n4   0     0      0    0    0        0    0   0     0     0     ...        \n\n   zombies  zombiethe  zone  zonealarm  zones  zoom  zooropa  zubair  zurich  \\\n0        0          0     0          0      0     0        0       0       0   \n1        0          0     0          0      0     0        0       0       0   \n2        0          0     0          0      0     0        0       0       0   \n3        0          0     0          0      0     0        0       0       0   \n4        0          0     0          0      0     0        0       0       0   \n\n   zvyagintsev  \n0            0  \n1            0  \n2            0  \n3            0  \n4            0  \n\n[5 rows x 31492 columns]\n\n    00  0001  00051  002  003  004secs  007   01  0100  0130     ...       \\\n0  0.0   0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n1  0.0   0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n2  0.0   0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n3  0.0   0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n4  0.0   0.0    0.0  0.0  0.0      0.0  0.0  0.0   0.0   0.0     ...        \n\n   zombies  zombiethe  zone  zonealarm  zones  zoom  zooropa  zubair  zurich  \\\n0      0.0        0.0   0.0        0.0    0.0   0.0      0.0     0.0     0.0   \n1      0.0        0.0   0.0        0.0    0.0   0.0      0.0     0.0     0.0   \n2      0.0        0.0   0.0        0.0    0.0   0.0      0.0     0.0     0.0   \n3      0.0        0.0   0.0        0.0    0.0   0.0      0.0     0.0     0.0   \n4      0.0        0.0   0.0        0.0    0.0   0.0      0.0     0.0     0.0   \n\n   zvyagintsev  \n0          0.0  \n1          0.0  \n2          0.0  \n3          0.0  \n4          0.0  \n\n[5 rows x 31491 columns]\n{'said'}\nFalse\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Training and testing models with CountVectorizer"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import the necessary modules\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\nnb_cv_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_cv_classifier.fit(count_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_cv_classifier.predict(count_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred, labels=['entertainment', 'politics', 'business', 'sports', 'tech'])\nprint(cm)\n",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.9816971713810316\n[[121   1   0   0   3]\n [  0 105   1   0   0]\n [  0   2 131   0   2]\n [  0   1   0 127   0]\n [  0   1   0   0 106]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Training and testing the model with TfidfVectorizer"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_tfidf_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_tfidf_classifier.fit(tfidf_train, y_train)\n\n# Create the predicted tags: pred\npred = nb_tfidf_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test, pred)\nprint(score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test, pred, labels=['entertainment', 'politics', 'business', 'sports', 'tech'])\nprint(cm)\n",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.961730449251248\n[[111   2   3   5   4]\n [  0 105   1   0   0]\n [  0   1 132   0   2]\n [  0   0   0 128   0]\n [  0   1   3   1 102]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Improving your model\n"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create the list of alphas: alphas\nalphas = np.arange(0, 1, .1)\n\n# Define train_and_predict()\ndef train_and_predict(alpha):\n    # Instantiate the classifier: nb_classifier\n    nb_cv_lassifier = MultinomialNB(alpha=alpha)\n    # Fit to the training data\n    nb_cv_lassifier.fit(count_train, y_train)\n    # Predict the labels: pred\n    pred = nb_cv_lassifier.predict(count_test)\n    # Compute accuracy: score\n    score = metrics.accuracy_score(y_test, pred)\n    return score\n\n# Iterate over the alphas and print the corresponding score\nbest_alfa_score = {}\nfor alpha in alphas:\n    best_alfa_score[alpha] = train_and_predict(alpha)\n    print('Alpha: ', alpha)\n    print('Score: ', train_and_predict(alpha))\n    print()\nbest_score = max(best_alfa_score, key=best_alfa_score.get)\n\nprint(\"Best alfa: \", best_score, \"best accuracy: \",best_alfa_score[best_score])",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n  'setting alpha = %.1e' % _ALPHA_MIN)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Alpha:  0.0\nScore:  0.9733777038269551\n\nAlpha:  0.1\nScore:  0.9850249584026622\n\nAlpha:  0.2\nScore:  0.9850249584026622\n\nAlpha:  0.30000000000000004\nScore:  0.9883527454242929\n\nAlpha:  0.4\nScore:  0.9866888519134775\n\nAlpha:  0.5\nScore:  0.9866888519134775\n\nAlpha:  0.6000000000000001\nScore:  0.9866888519134775\n\nAlpha:  0.7000000000000001\nScore:  0.9833610648918469\n\nAlpha:  0.8\nScore:  0.9833610648918469\n\nAlpha:  0.9\nScore:  0.9816971713810316\n\nBest alfa:  0.30000000000000004 best accuracy:  0.9883527454242929\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Create the list of alphas: alphas\nalphas = np.arange(0, 1, .1)\n\n# Define train_and_predict()\ndef train_and_predict(alpha):\n    # Instantiate the classifier: nb_classifier\n    nb_tfidf_classifier = MultinomialNB(alpha=alpha)\n    # Fit to the training data\n    nb_tfidf_classifier.fit(tfidf_train, y_train)\n    # Predict the labels: pred\n    pred = nb_tfidf_classifier.predict(tfidf_test)\n    # Compute accuracy: score\n    score = metrics.accuracy_score(y_test, pred)\n    return score\n\n# Iterate over the alphas and print the corresponding score\nbest_alfa_score = {}\nfor alpha in alphas:\n    best_alfa_score[alpha] = train_and_predict(alpha)\n    print('Alpha: ', alpha)\n    print('Score: ', train_and_predict(alpha))\n    print()\nbest_score = max(best_alfa_score, key=best_alfa_score.get)\n\nprint(\"Best alfa: \", best_score, \"best accuracy: \",best_alfa_score[best_score])",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n  'setting alpha = %.1e' % _ALPHA_MIN)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Alpha:  0.0\nScore:  0.9683860232945092\n\nAlpha:  0.1\nScore:  0.9850249584026622\n\nAlpha:  0.2\nScore:  0.9833610648918469\n\nAlpha:  0.30000000000000004\nScore:  0.978369384359401\n\nAlpha:  0.4\nScore:  0.9767054908485857\n\nAlpha:  0.5\nScore:  0.9750415973377704\n\nAlpha:  0.6000000000000001\nScore:  0.9717138103161398\n\nAlpha:  0.7000000000000001\nScore:  0.9717138103161398\n\nAlpha:  0.8\nScore:  0.9700499168053245\n\nAlpha:  0.9\nScore:  0.9650582362728786\n\nBest alfa:  0.1 best accuracy:  0.9850249584026622\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Inspecting the model\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get the class labels: class_labels\nclass_labels = nb_classifier.classes_\n\n# Extract the features: feature_names\nfeature_names = tfidf_vectorizer.get_feature_names()\n\n# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\nfeat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n\n# Print the first class label and the top 20 feat_with_weights entries\nprint(class_labels[0], feat_with_weights[:20])\n\n# Print the second class label and the bottom 20 feat_with_weights entries\nprint(class_labels[1], feat_with_weights[-20:])\n",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": "business [(-10.448122083212702, '00'), (-10.448122083212702, '0001'), (-10.448122083212702, '00051'), (-10.448122083212702, '002'), (-10.448122083212702, '004secs'), (-10.448122083212702, '007'), (-10.448122083212702, '0100'), (-10.448122083212702, '0130'), (-10.448122083212702, '019secs'), (-10.448122083212702, '0227'), (-10.448122083212702, '028'), (-10.448122083212702, '030'), (-10.448122083212702, '0300'), (-10.448122083212702, '0305'), (-10.448122083212702, '0469'), (-10.448122083212702, '050505'), (-10.448122083212702, '053'), (-10.448122083212702, '053bn'), (-10.448122083212702, '0605'), (-10.448122083212702, '0700')]\nentertainment [(-8.67461810361125, 'quarter'), (-8.651653481004878, 'new'), (-8.648752865809342, 'rate'), (-8.64696044438273, 'rise'), (-8.644716473046667, 'dollar'), (-8.587806528567564, 'government'), (-8.533053369227774, '2004'), (-8.473116806427768, 'firm'), (-8.467405603196003, 'shares'), (-8.413716338924312, 'sales'), (-8.402486142334473, 'company'), (-8.376341386199387, 'mr'), (-8.37626372159095, 'prices'), (-8.35993645510219, 'economic'), (-8.251525696671298, 'year'), (-8.216197751006597, 'market'), (-8.198656233529181, 'bank'), (-8.179291431259, 'economy'), (-8.174784209298222, 'oil'), (-8.14285294237519, 'growth')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### Test on new dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Import new data\nos.chdir('/home/nbuser/library/1. Classifier/3. Exploratory Data Analysis')\nnew_data = pd.read_csv('news_df_validation.csv')\nnew_data.head()",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>title</th>\n      <th>news_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>214.txt</td>\n      <td>Mansfield 0-1 Leyton Orient</td>\n      <td>An second-half goal from Andy Scott condemned ...</td>\n      <td>sports</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>303.txt</td>\n      <td>Film production 'falls' 40% in UK</td>\n      <td>The number of British films produced in the UK...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>083.txt</td>\n      <td>Hague 'given up' his PM ambition</td>\n      <td>Former Conservative leader William Hague says ...</td>\n      <td>politics</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>190.txt</td>\n      <td>SA return to Mauritius</td>\n      <td>Top seeds South Africa return to the scene of ...</td>\n      <td>sports</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>103.txt</td>\n      <td>Minimum rate for foster parents</td>\n      <td>Foster carers are to be guaranteed a minimum a...</td>\n      <td>politics</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "  file_name                              title  \\\n0   214.txt        Mansfield 0-1 Leyton Orient   \n1   303.txt  Film production 'falls' 40% in UK   \n2   083.txt   Hague 'given up' his PM ambition   \n3   190.txt             SA return to Mauritius   \n4   103.txt    Minimum rate for foster parents   \n\n                                           news_text       category  \n0  An second-half goal from Andy Scott condemned ...         sports  \n1  The number of British films produced in the UK...  entertainment  \n2  Former Conservative leader William Hague says ...       politics  \n3  Top seeds South Africa return to the scene of ...         sports  \n4  Foster carers are to be guaranteed a minimum a...       politics  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Count Vectorizer on test dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def new_data_count_vec_predict(test_new_data):\n    clean_df = clean_all_text(test_new_data)\n    nb_cv_classifier = MultinomialNB(alpha=0.3)\n    nb_cv_classifier.fit(count_train, y_train)\n    count_test = count_vectorizer.transform(test_new_data['news_text'])\n    new_cv_pred = nb_cv_classifier.predict(count_test)\n    return new_cv_pred",
      "execution_count": 58,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def print_new_data_pred(test_data, index):\n    new_data_cat = new_data['category']\n    print(new_data_count_vec_predict(test_data)[index],' == ', new_data_cat[index])",
      "execution_count": 59,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print_new_data_pred(new_data, 28)",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": "entertainment  ==  entertainment\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count_vec_new_pred = new_data_count_vec_predict(new_data)",
      "execution_count": 61,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### TFIDF on new dataset test"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def new_data_ftidf_vec_predict(test_new_data):\n    clean_df = clean_all_text(test_new_data)\n    nb_tfidf_classifier = MultinomialNB(0.1)\n    nb_tfidf_classifier.fit(tfidf_train, y_train)\n    ftidf_test = tfidf_vectorizer.transform(test_new_data['news_text'])\n    new_tfidf_pred = nb_tfidf_classifier.predict(ftidf_test)\n    return new_tfidf_pred",
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tfidf_new_pred = new_data_ftidf_vec_predict(new_data)",
      "execution_count": 63,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print(tfidf_new_pred[112])",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": "sports\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### End the scooooore iiiiiiiis: "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count_vec_score = metrics.accuracy_score(count_vec_new_pred, new_data['category'])\nprint('Count vectorize score: ',round(count_vec_score, 4))\nprint()\ntfidf_vec_score = metrics.accuracy_score(new_data_ftidf_vec_predict(new_data), new_data['category'])\nprint('TF-IDF score: ',round(tfidf_vec_score, 4))",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Count vectorize score:  0.973\n\nTF-IDF score:  0.964\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### CV: 0.9775 <br><br>TFIDF: 0.973"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Saving models to pickle file"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Import pickle\nimport pickle",
      "execution_count": 66,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# save count_vector model to disk\n\nos.chdir('/home/nbuser/library/1. Classifier/4. Feature Engineering')\nfilename = 'count_vec_model.pkl'\npickle.dump(nb_cv_classifier, open(filename, 'wb'))\n \n \n# load the model from disk\n#loaded_model = pickle.load(open(filename, 'rb'))",
      "execution_count": 69,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# save tf-idf_vector model to disk\n\nfilename = 'tfidf_vec_model.pkl'\npickle.dump(nb_tfidf_classifier, open(filename, 'wb'))\n \n ",
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Test on BBC live news"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#! run  '/home/nbuser/library/2. News Scrapper/BBC news scrapper with NewsApi.ipynb'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "BBC_live_news_df = pd.read_csv('/home/nbuser/library/2. News Scrapper/BBC_LIVE_NEWS.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Predict live news class"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count_vec_live_pred = new_data_count_vec_predict(BBC_live_news_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tfidf_live_pred = new_data_ftidf_vec_predict(BBC_live_news_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "BBC_live_news_df['CV Pred Category'] = count_vec_BBC_live_pred\nBBC_live_news_df['TF-IDF Pred Category'] = tfidf_live_pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "BBC_live_news_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Save the BBC live news prediction file"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def saving_file(file, file_name, save_dir):\n    file.to_csv(os.path.join(save_dir,file_name), index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "save_dir = '/home/nbuser/library/2. News Scrapper'\nfile = BBC_all_news_df\nfile_name = 'BBC_LIVE_NEWS_CLASSES.csv'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "saving_file(file, file_name, save_dir)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}