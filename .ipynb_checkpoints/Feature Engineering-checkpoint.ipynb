{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Word Embeddings - We'll use TF-IDF Vector\n\n\n- a. __Special character cleaning__: special characters such as “\\n” double quotes must be removed from the text since we aren’t expecting any predicting power from them.\n- b. __Upcase/downcase__: we would expect, for example, “Book” and “book” to be the same word and have the same predicting power. For that reason we have downcased every word.\n- c. __Punctuation signs__: characters such as “?”, “!”, “;” have been removed.\n- d. __Possessive pronouns__: in addition, we would expect that “Trump” and “Trump’s” had the same predicting power.\n- e. __Stemming or Lemmatization__: stemming is the process of reducing derived words to their root. Lemmatization is the process of reducing a word to its lemma. The main difference between both methods is that lemmatization provides existing words, whereas stemming provides the root, which may not be an existing word. We have used a Lemmatizer based in WordNet.\n- f. __Stop words__: words such as “what” or “the” won’t have any predicting power since they will presumably be common to all the documents. For this reason, they may represent noise that can be eliminated. We have downloaded a list of English stop words from the nltk package and then deleted them from the corpus.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Loading file from path\ndef loading_file():\n    file_dir = '/home/nbuser/library/1. Classifier/3. Exploratory Data Analysis'        \n    file_list = glob.glob(file_dir + '/*.csv')\n    csv_file = file_list[0]\n    return csv_file\n\n# Import file imto Pandas DataFrame\ndef importing_file(csv_file):\n    df = pd.read_csv(csv_file, sep=\",\")\n    return df\n\n# Saving path\ndef saving_file(file, file_name, save_dir):\n    file.to_csv(os.path.join(save_dir,file_name))\n",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Importing  file"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Importing file + Loading  file\nnews_df = importing_file(loading_file())\n\n# Top 5 records\nnews_df.head()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>title</th>\n      <th>news_text</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>348.txt</td>\n      <td>Berlin celebrates European cinema</td>\n      <td>Organisers say this year's Berlin Film Festiva...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>139.txt</td>\n      <td>U2 to play at Grammy awards show</td>\n      <td>Irish rock band U2 are to play live at the Gra...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>125.txt</td>\n      <td>Snow Patrol feted at Irish awards</td>\n      <td>Snow Patrol were the big winners in Ireland's ...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>267.txt</td>\n      <td>T in the Park sells out in days</td>\n      <td>Tickets for Scotland's biggest music festival ...</td>\n      <td>entertainment</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>311.txt</td>\n      <td>Corbett attacks 'dumbed-down TV'</td>\n      <td>Ronnie Corbett has joined fellow comedy stars ...</td>\n      <td>entertainment</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "  file_name                              title  \\\n0   348.txt  Berlin celebrates European cinema   \n1   139.txt   U2 to play at Grammy awards show   \n2   125.txt  Snow Patrol feted at Irish awards   \n3   267.txt    T in the Park sells out in days   \n4   311.txt   Corbett attacks 'dumbed-down TV'   \n\n                                           news_text       category  \n0  Organisers say this year's Berlin Film Festiva...  entertainment  \n1  Irish rock band U2 are to play live at the Gra...  entertainment  \n2  Snow Patrol were the big winners in Ireland's ...  entertainment  \n3  Tickets for Scotland's biggest music festival ...  entertainment  \n4  Ronnie Corbett has joined fellow comedy stars ...  entertainment  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df['news_text'].apply(lambda x: len(x.split(' '))).sum()",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "766239"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### NLP Pipeline\n#### 1 Word Tokenization, 2 Part of Speech, 3 Lemmatization, 4 Stemming, 5 Stop words, 6 Sentence, Tokenization"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Text Classification With Scikit-Learn\n\nhttps://e-string.com/articles/text-classification-with-sklearn/"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Clean the text column"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\ndef clean_text(x):\n    pattern = r'[^a-zA-z0-9\\s]'\n    text = re.sub(pattern, '', x)\n    return text",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def clean_all_text(df, column):\n    for raw in df[column]:\n        cleantext = clean_text(raw)\n        df.loc[raw, column] = cleantext\n    return df\n",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_text = news_df.loc[:13, 'news_text']\n\ntest_clean = clean_all_text(test_text, 'news_text')\nprint(test_clean)",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'news_text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: an integer is required",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-e30922c509a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'news_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_all_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'news_text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-9840dfbdff37>\u001b[0m in \u001b[0;36mclean_all_text\u001b[0;34m(df, column)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_all_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mcleantext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleantext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'news_text'"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Bag of words, n-grams, tf-idf\nhttps://github.com/RaRe-Technologies/movie-plots-by-genre/blob/master/Document%20classification%20with%20word%20embeddings%20tutorial.ipynb"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Implementing multi-class text classification with Doc2Vec\nhttps://towardsdatascience.com/implementing-multi-class-text-classification-with-doc2vec-df7c3812824d"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Multi-Class text classification using XGBoost & others, Doc2Vec, TfIdf\n\nhttps://github.com/avisheknag17/public_ml_models/blob/master/bbc_articles_text_classification/notebook/text_classification_xgboost_others.ipynb"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Gensim NLTK\nhttps://github.com/DistrictDataLabs/PyCon2016/tree/master/notebooks/tutorial"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk\nnltk.download('punkt')",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# We remove stop-words and use NLTK tokenizer then limit our vocabulary to 3k most frequent words.\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            if len(word) < 2:\n                continue\n            tokens.append(word.lower())\n    return tokens",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import necessary modules\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n  \n\nfor raw in news_df.loc[:13,'news_text']:\n    text = raw\n    # Split scene_one into sentences: sentences\n    sentences = sent_tokenize(text)\n\n    # Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n    tokenized_sent = word_tokenize(sentences[3])\n\n    # Make a set of unique tokens in the entire scene: unique_tokens\n    unique_tokens = set(word_tokenize(text))\n\n#!!!!!!!!!!!!!!!!!!!!!!!\n#news['tokens'] = news_df.loc[raw, tokens]\nprint(unique_tokens)\n",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'Wham', 'common', 'listed', 'Know', 'na', 'that', 'Moped', '?', 'Surprisingly', 'says', 'reference', 'chart', '8,000', 'commissioned', '``', 'There', 'no', 'This', '75', 'for', 'said', 'Sir', 'earth', 'at', 'rightful', 'into', 'formulaic', 'all', 'our', 'by', 'on', 'should', 'track', 'formula', 'be', 'bells', 'The', 'a', 'Band', 'number', 'Everybody', 'single', 'sleigh', 'so', 'chance', '.', 'top', 'British', 'times', 'Roberts', 'Wine', 'peace', 'book', 'song', 'remake', 'Last', 'recipe', 'this', 'musical', 'is', 'whole', 'linking', 'Aid', 'They', 'revealed', 'Merry', 'set', \"''\", ',', 'has', 'Number', 'recent', 'elements', 'recording', 'Mr', 'group', 'wishes', 'combine', 'Sunday', 'also', 'Santa', 'office', 'big', 'Cliff', 'Vs', 'title', '20', 'Slade', 'Mistletoe', 'Singles', '-', 'ultimate', 'Christmas', 'years', 'A', 'but', 'place', 'It', 'prank', 'Do', 'as', 'the', 'help', 'Have', 'called', 'Gon', 'Father', 'to', 'charity', 'one', 'lots', 'festive', 'there', 'parties', 'in', 'first', 'performers', 'Hit', 'analysts', 'Albums.The', 'and', 'editor', 'among', 'history', 'confirmed', 'One', 'charts', 'hits', 'David', 'ones', 'his', 'over', 'No', 'includes', 'Big', 'element', 'create', 'include', 'children', 'choir', 'crack', 'Richards', \"'s\", 'of', '1', 'bible', 'these', 'airplay', 'using', 'been', 'nearly', \"'\", 'are'}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def print_plot(index):\n    example = news_df[news_df.index == index][['news_text', 'category']].values[0]\n    if len(example) > 0:\n        print(example[0])\n        print('Genre:', example[1])",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print_plot(13)",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": "US rap star 50 Cent has said he has thrown protege The Game out of his G-Unit gang in a feud that has apparently involved two shootings.In a radio interview on Monday, 50 Cent said the newcomer was disloyal in conflicts with other rappers. A man was shot in the thigh outside New York's Hot 97 studios while 50 Cent was on air. More shots were fired outside his management offices two hours later. 50 Cent appeared on The Game's debut album, which was number one in the US. 50 Cent, whose second album is about to be released after his debut made him one of hip-hop's biggest stars, has been involved in recent rivalries with fellow artists including Fat Joe, Nas and Jadakiss.He has claimed credit for the success of The Game, who has become the hottest new star on the rap scene. Both were drug dealers and were shot before turning to music.In an interview with Hot 97 on Saturday, The Game described some of 50 Cent's rivals as \"my friends\" and said he would not turn on them. \"Nas is one of my friends, and Jada's really a homie,\" he said. \"50's beef is 50's beef and I really don't know where all this stems from.\" When 50 Cent appeared on the same station two days later, he said The Game was no longer a member of G-Unit. \"Every record he's selling is based on me being on his record with him,\" he said. When the shooting took place outside the studio, the interview was ended and the rapper was escorted out of the building by security personnel.An unidentified 24-year-old Los Angeles man is stable with a gunshot wound to the upper thigh. Police say The Game's associates may have heard the interview and gone to the studio, where they confronted 50 Cent's entourage. Officers are also investigating a later shooting in which eight bullets were fired into the door of 50 Cent's management company, Violator. No arrests have been made in relation to either incident. 50 Cent's second album, The Massacre, is released on Thursday, five weeks after The Game's debut, Documentary, went to number one.Elliott Wilson, editor-in-chief of hip-hop magazine XXL, said the feud would boost publicity for 50 Cent's release. \"It helps him obviously in terms of exposure. You can't ask for better promotion,\" he said. But he added: \"I think he's making more and more enemies. \"You definitely feel like is he doing too much of a Tupac spiral, like me against the world. You bring more people wanting to see you fail.\" Tupac Shakur was shot dead in 1996.\nGenre: entertainment\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print_plot(209)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}